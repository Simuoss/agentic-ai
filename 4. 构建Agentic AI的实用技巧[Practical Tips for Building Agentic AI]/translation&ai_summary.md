# 课程字幕翻译

1. ## **Evaluations (evals)** —— 评估（evals）

在本单元中，我想与大家分享构建**代理式人工智能工作流程 (agentic ****AI**** workflows)** 的实用技巧。

0:04

我希望这些技巧能让您比一般的开发人员**更有效地**构建此类系统。

0:10

我发现，在开发一个代理式人工智能系统时，很难**提前知道**它会在哪些地方运行良好，又会在哪些地方表现不佳，

0:16

因此也就难以确定应该将精力集中在哪里。

0:21

所以，一个非常普遍的建议是，**先尝试构建一个哪怕是快速而粗糙的系统**，

0:27

这样您就可以试运行它，进行观察，看看它可能在哪些方面还未达到您的期望，

0:34

然后就能**更有针对性地**投入精力去进一步开发它。

0:40

相比之下，我发现花好几个星期坐着**空泛地理论化和假设**如何构建系统，有时效果并不那么好。

0:46

通常更好的做法是，以一种安全、合理的方式，快速地构建一个系统，它不会泄露数据，

0:53

以一种**负责任的方式**去做，但就是要**快速地构建一些东西**，这样您就可以观察它，然后利用这个**初始原型**来确定**后续开发的优先级**并进行尝试。

0:58

让我们从一个例子开始，看看在您构建了一个原型之后可能会发生什么。

1:10

我想用您之前看过的**发票处理工作流程**作为第一个例子，

1:16

它的任务是**提取四个必填字段**，然后将其**保存到数据库记录**中。

1:22

在构建了这样一个系统之后，您可能会做的一件事是**找一些发票**，也许是 10 到 20 张，

1:28

然后**逐一查看**它们，观察系统的**输出**，看看哪些地方运行良好，以及是否存在任何错误。

1:33

假设您查看了 20 张发票，发现**发票 1** 没问题，输出看起来是正确的。

1:38

对于**发票 2**，它可能**混淆了发票的日期**（即发票开具的日期）和**发票的到期日**，

1:43

而在这个任务中，我们想要提取**到期日**，以便能**按时付款**。

1:48

因此，我可能会在文档或电子表格中记下，对于发票 2，日期被混淆了。

1:53

也许发票 3 没问题，发票 4 也没问题，依此类推。

1:58

但当我查看这个例子时，我发现**有很多例子**都出现了**日期混淆**的问题。

2:03

因此，正是基于对像这样的多个例子的查看，您可能会得出结论：在这种情况下，一个常见的**错误模式**是系统在处理**日期**时遇到了困难。

2:10

在这种情况下，您可能会考虑的其中一件事当然是找出**如何改进系统**，使其能更好地提取到期日，

2:17

但同时也可以**编写一个评估 (eval)** 来衡量它提取到期日的**准确性**。

2:23

相比之下，如果您发现系统是**错误地提取了开票人地址**，

2:29

谁知道呢，也许您的开票人有不寻常的名字，所以系统在处理开票人时有困难，

2:35

又或者，特别是如果您的开票人是**国际的**，他们的名字可能**并非全部是英文字母**，

2:40

那么您可能就会转而**专注于构建一个针对开票人地址的评估**。

2:45

因此，快速构建一个粗糙的系统并查看输出之所以如此有用，其中一个原因就是它甚至能帮助您**决定将最大的精力投入到评估什么**。

2:51

现在，如果您决定要修改您的系统以**提高**它提取发票**到期日的准确性**，

3:03

那么为了跟踪进度，一个好主意可能是**创建一个评估 (evaluation) 或一个 eval** 来衡量日期提取的准确性。

3:09

实现这一点可能有多种方法，但让我与您分享一下**我可能会怎么做**。

3:14

为了创建一个**测试集或评估集**，我可能会找到 10 到 20 张发票，然后**手动写下**它们的到期日。

3:20

所以，也许一张发票的到期日是 2025 年 8 月 20 日，我会将其写成标准的**年-月-日**格式。

3:25

然后为了方便之后在代码中进行评估，我可能会编写对 LLM 的**提示**，告诉它**始终将到期日格式化**为这种**年-月-日**格式。

3:33

有了这个，我就可以编写代码来**提取** LLM 输出的**唯一日期**，即到期日，

3:39

因为这是我们**唯一关心**的日期。

3:46

所以，这是一个**正则表达式**、**模式匹配**，匹配四位数字的年份、两位数字的月份、两位数字的日期，然后将其提取出来。

3:51

然后我就可以简单地编写代码来测试**提取出的日期是否等于实际的日期**，也就是我写下的**基本事实 (ground truth) 注释**。

3:56

因此，有了**大约 20 张发票**的评估集，我就可以构建并进行更改，看看系统**正确提取日期的百分比**是否在随着我对提示或系统其他部分的调整而**逐渐上升**。

4:06

所以，总结一下我们目前所看到的内容：我们**构建**一个系统，然后**查看输出**以发现它可能表现不尽人意的地方，例如**到期日错误**。

4:18

接着，为了推动对这个重要输出的改进，我们**建立一个小型评估**，比如只有 20 个例子，来帮助我们**跟踪进度**。

4:29

这让我可以**回去调整提示**、**尝试不同的算法**等等，以观察我是否能**提高到期日准确性的这个指标**。

4:35

这就是**改进代理式人工智能工作流程**通常会给人的感觉。

4:46

**查看输出**，**发现哪里出错了**，然后如果您知道如何修复它，就**直接修复**。

4:57

但如果需要一个**更长的改进过程**，那么就**建立一个评估**，并用它来**推动进一步的开发**。

5:01

另一个需要考虑的问题是，如果在工作了一段时间后，您认为**最初的 20 个例子不够好**，

5:05

也许它们**没有涵盖**您想要的所有情况，或者**20 个例子实在太少**了，

5:10

那么您**随时可以随着时间的推移增加评估集**，以确保它能更好地反映您个人对系统性能是否足够满意的**判断**。

5:15

这只是一个例子。对于**第二个例子**，让我们看看如何构建一个**营销文案助理**来撰写 Instagram 的**标题 (caption)**，

5:25

为了简洁起见，假设我们的营销团队告诉我们，他们希望标题**最多 10 个词**长。

5:30

所以，我们会有**产品图片**，比如我们想要营销的一副太阳镜，

5:35

然后有一个**用户查询**，比如“请写一个推销这副太阳镜的标题”，

5:40

接着让一个 **LLM**，或者**大型多模态模型**，**分析图像和查询**，并**生成**太阳镜的描述。

5:45

营销文案助理可能出问题的地方有很多，

5:52

但假设您查看了输出，发现生成的文案或文本**大部分听起来没问题**，但**有时就是太长了**。

5:58

比如，对于太阳镜的输入，生成了 17 个词；如果是咖啡机，没问题；时尚，没问题；蓝衬衫，14 个词；搅拌机，11 个词。

6:03

所以在这个例子中，看起来 LLM 在**遵守长度指南**方面遇到了困难。

6:08

再次强调，营销文案助理可能出现问题的环节有很多。

6:13

但如果您发现它在**输出长度**上遇到困难，您可能就会**构建一个评估来跟踪这个问题**，

6:18

以便进行改进，并确保它在**遵守长度指南**方面变得越来越好。

6:24

所以，为了创建一个**衡量文本长度的评估**，您可能会**创建一个测试任务集**，

6:28

标注一副太阳镜、一台咖啡机等等，也许**创建 10 到 20 个例子**。

6:33

然后，您将**运行**每个例子通过您的系统，并编写代码来**衡量输出的词数**。

6:39

所以，这是**测量一段文本词数的 Python 代码**。

6:44

最后，您将**比较**生成的文本的长度与 **10 个词的限制**。

6:49

所以，如果词数等于 10，那么“我正确了，计数加一”。

6:56

这个例子与前一个**发票处理例子**的一个区别是，这里**没有每个例子的基本事实 (ground truth)**。

7:02

**目标就是 10**，对每个例子都一样。

7:10

相比之下，对于发票处理的例子，我们必须**生成一个定制的目标标签**，即**发票的正确到期日**，我们正在用输出**对照**这个每个例子的基本事实进行测试。

7:15

我知道我用了一个非常简单的工作流程来生成这些标题，但**这些类型的评估**也可以应用于**更复杂的生成工作流程**。

7:22

让我再谈谈**最后一个例子**，在这个例子中我们将**重新审视**我们一直在看的**研究代理**。

7:38

如果您观察研究代理在**不同输入提示**上的输出，

7:43

假设当您要求它撰写一篇关于**黑洞科学最新突破**的文章时，您发现它**遗漏**了一些备受关注的、有很多新闻报道的**高知名度成果**。

7:49

所以这是一个**不尽人意的结果**。

7:55

或者如果您要求它研究在西雅图是**租房还是买房**，嗯，它似乎做得不错。

8:01

或者研究**用于水果采摘的机器人技术**。嗯，它**没有提到**一家**领先的设备公司**。

8:07

因此，基于这个评估，看起来它**有时会遗漏**一个**人类专家撰稿人会捕捉到的非常重要的点**。

8:11

所以，我就会**创建一个评估**来衡量它**捕获最重要观点的频率**。

8:16

例如，您可能会想出**一些关于黑洞、机器人采摘等的示例提示**。

8:22

然后，对于每一个提示，想出**三到五个**针对这些主题的**黄金标准讨论点 (gold standard discussion points)**。

8:29

请注意，这里**我们确实有每个例子的标注**，因为**黄金标准讨论点**，即最重要的讨论点，**对于每个例子都是不同的**。

8:34

有了这些**基本事实的标注**，您就可以使用 **LLM**** 作为评判者 (LLM-as-a-judge)** 来**统计**提到了多少个**黄金标准讨论点**。

8:40

所以，一个**示例提示**可能是说：“确定所提供的文章中**存在**五个黄金标准讨论点中的**多少个**。”

8:45

您可以提供可选的提示、文章文本、黄金标准点等等，

8:51

并让它返回一个 **JSON**** 对象**，其中包含两个字段：一个**分数**（0 到 5，表示提到了多少点），以及一个**解释**。

8:56

这使您能够为评估集中的每个提示**获得一个分数**。

9:01

在这个例子中，我使用 **LLM 作为评判者**来统计提到了多少个讨论点，

9:07

因为讨论这些讨论点的方式有**太多种**了，

9:12

所以一个**正则表达式**或一个用于简单模式匹配的代码可能**效果不佳**，

9:17

这就是为什么您可能使用 **LLM**** 作为评判者**，并将其视为一个**稍微更主观的评估**，来判断例如**事件视界**是否得到了充分提及。

9:22

这是您**构建评估的第三个例子**。

9:35

为了思考**如何为您的应用程序构建评估**，您构建的评估通常必须**反映**您看到或担心的应用程序中可能**出错的任何地方**。

10:04

结果表明，广义上讲，评估有两个**维度 (axes)**。

10:10

**顶部的维度**是您**评估输出的方式**。在某些情况下，您通过编写带有**客观评估 (objective evals)** 的**代码**来评估它，

10:18

有时您使用 **LLM**** 作为评判者**来进行**更主观的评估 (subjective evals)**。

10:26

**另一个维度**是您**是否拥有每个例子的基本事实 (per-example ground truth)**。

10:34

因此，在**检查发票日期提取**时，我们**编写代码**来评估是否获得了**实际日期**，

10:43

这**具有每个例子的基本事实**，因为每张发票都有一个**不同的实际日期**。

10:49

但在我们**检查营销文案长度**的例子中，**每个例子**都有一个 **10 个词的长度限制**，

10:55

所以那个问题**没有每个例子的基本事实**。

11:02

相比之下，对于**统计黄金标准讨论点**，**有每个例子的基本事实**，因为每篇文章都有**不同的重要讨论点**。

11:07

但我们使用 **LLM**** 作为评判者**来阅读文章，看看这些主题是否得到了**充分提及**，

11:13

因为提及这些讨论点的方式有**太多种**了。

11:17

这四个象限中的**最后一个**将是**LLM**** 作为评判者，且没有每个例子的基本事实**。

11:23

我们看到的一个例子是，如果您**根据评分标准 (****rubric****) 对图表进行评分**。

11:30

这是我们在查看咖啡机销售可视化时，如果您要求它**根据评分标准创建图表**，例如是否有**清晰的****坐标轴****标签**等等，

11:35

那么**每张图表**都有**相同的评分标准**，这就是使用 **LLM**** 作为评判者，但没有每个例子的基本事实**。

11:40

所以我发现这个**二乘二的网格**可能是一个有用的方式，来思考您可以为您的应用程序构建的**不同类型的评估**。

11:46

顺便说一句，这些有时也称为**端到端评估 (end-to-end evals)**，因为**一端**是**输入端**，即**用户查询提示**，

11:51

而**另一端**是**最终输出**。因此，所有这些都是针对整个**端到端系统性能**的评估。

11:56

所以，为了结束这个视频，我想分享一些关于**设计端到端评估的最后建议**。

12:08

首先，**快速而粗糙的评估**适合入门。

12:13

我感觉我看到很多团队几乎**陷入瘫痪**，因为他们认为构建评估是一个**巨大的、耗时数周的工程**，

12:20

所以他们开始行动的时间比理想中要**更长**。

12:25

但我认为，就像您**迭代**一个代理式工作流程并随着时间推移使其变得更好一样，您也应该计划**迭代您的评估**。

12:32

所以，如果您将 10、15、20 个例子作为**评估的初稿**，并编写一些代码或尝试提示 **LLM**** 作为评判者**，

12:37

**先做点什么**来获得一些**指标**，这些指标可以**补充人眼**对输出的观察，

12:44

然后**两者结合**可以推动您的决策。

12:49

随着评估**随着时间推移变得更加复杂**，您可以将**越来越多的信任**转移到**基于指标的评估**上，

12:54

而不是每次调整某个提示时都需要**阅读数百个输出**。

12:58

在您经历这个过程时，您很可能会发现**继续改进您的评估**的方法。

13:03

所以，如果您一开始有 20 个例子，您可能会遇到您的评估**未能捕捉到您对哪个系统更好的判断**的情况。

13:15

所以，也许您更新了系统，您观察它，感觉它**肯定**工作得更好了，但您的评估**未能显示**新系统获得了更高的分数。

13:22

如果发生这种情况，那通常是一个**机会**，可以去**收集一个更大的评估集**，

13:28

或者**改变您评估输出的方式**，使其能更好地**对应**您对哪个系统**实际上工作得更好**的判断。

13:34

所以您的评估会**随着时间而改进**。

13:40

最后，关于使用评估来**获取下一步工作灵感**方面，

13:45

许多代理式工作流程正被用于**自动化人类可以完成的任务**。

13:50

因此，我发现对于此类应用程序，我会寻找**性能比不上专家人类**的地方，

13:56

这通常能给我**灵感**，让我知道**应该把精力集中在哪里**，或者**哪些类型的例子**我可以让我的代理式工作流程**比目前做得更好**。

14:02

所以我希望在您构建了那个**快速而粗糙的系统**之后，您能思考**何时开始**引入一些**评估**来**跟踪系统中潜在的问题方面**，

14:11

然后这将帮助您**推动系统的改进**。

14:18

除了帮助您推动改进之外，事实证明，有一种**评估方法**可以帮助您**聚焦**在您**整个代理式系统**中，**哪些组件最值得您投入注意力**？

14:23

因为代理式系统通常**有很多组成部分**。

14:34

那么**哪个部分**是您花时间去改进**最有成效**的呢？

14:40

事实证明，能够很好地做到这一点是**推动代理式工作流程高效开发**的一项非常重要的技能。

14:47

在下一个视频中，我想深入探讨这个话题。让我们继续下一个视频吧。

2. ## **Error analysis and prioritizing next steps** —— 错误分析与后续步骤优先级

假设您已经构建了一个代理式工作流程，但它的运行效果还**未达到您的预期**（顺便说一下，这种情况在我身上经常发生，我会快速搭建一个粗糙的系统，但它的表现不如我所愿），

0:04

那么问题是：您应该将精力**集中在哪里**才能使其更好？

0:09

事实证明，代理式工作流程有**许多不同的组件**，**改进某些组件**可能比改进其他组件**更有成效**。

0:14

因此，您**选择工作重点的技能**，对于您**提高系统改进速度**来说至关重要。

0:19

我发现，衡量一个团队效率和优秀程度的**最大预测指标之一**，就是他们是否能够推行一个**严谨的错误分析 (error analysis) 流程**，来指导他们将精力集中在哪里。

0:25

所以，这是一项重要的技能。让我们来看看**如何进行错误分析**。

0:30

在研究代理的例子中，我们在上一个视频中进行了错误分析，发现它在撰写关于特定主题的文章时，经常会**遗漏**一个人类专家会提到的**关键点**。

0:41

现在您已经发现了这个问题，即“有时会遗漏关键点”，您**如何知道应该从何处着手改进**？

0:47

事实证明，在这个工作流程的**许多不同步骤中**，**几乎任何一个**都可能导致“遗漏关键点”这个问题。

0:54

例如，可能**第一个 ****LLM**** 生成的搜索词不够好**，导致它搜索了错误的内容，因此没有发现正确的文章。

1:00

或者，也许您使用的**网页****搜索引擎**本身就**不怎么样**。市面上有多种网页搜索引擎，实际上我自己的基础应用倾向于使用几种，但它们之间也有优劣之分。

1:06

或者，网页搜索本身没有问题，但当我们把网页搜索结果列表交给 LLM 时，**LLM 在选择下载哪几篇最好的文章时，可能做得不够好**。

1:12

在这个例子中，假设您能准确地抓取网页，那么网页抓取 (web fetch) 可能问题较少。但将网页内容输入给 LLM 后，**LLM 可能会忽略**我们抓取到的文档中的**某些要点**。

1:17

因此，事实证明，有些团队有时会**凭直觉**选择其中一个组件进行改进，有时这种方法有效，但有时可能会导致**数月的工作，而系统的整体性能却进展甚微**。

1:23

因此，与其凭直觉决定改进这众多组件中的哪一个，

1:31

我认为**更好地进行错误分析**来**深入理解工作流程的每一步**要有效得多。

1:39

特别是，我通常会**检查追踪 (traces)**，这意味着**每一步之后的中间输出**，以便了解**哪个组件的性能不达标 (subpar)**，

1:44

也就是比人类专家的表现差得多，因为这指明了**安全改进的空间**。

1:51

让我们看一个例子。如果我们要求研究代理撰写一篇关于**黑洞科学最新新闻**的文章，

1:56

它输出的搜索词可能如下：“搜索黑洞理论 爱因斯坦”、“事件视界望远镜 无线电”等等。

2:02

然后我会请一位**人类专家**查看这些搜索词，判断它们对于撰写关于黑洞科学最新发现的文章来说是否**合理**。

2:09

可能在这个例子中，专家会说：“这些网页搜索词看起来还可以，与我作为人类会做的事情**非常相似**。”

2:13

然后我再查看**网页搜索的输出**，观察返回的 **URL**。

2:20

网页搜索会返回许多不同的网页，其中一个返回的网页可能是“某小学生声称追踪到 30 年前的黑洞谜团 - 来自 Astro Kid News”。

2:27

这看起来**不像是一篇最严谨的、经过同行评审的文章**。

2:32

也许通过检查网页搜索返回的所有文章，您会得出结论：它返回了**太多博客或大众媒体类型的文章**，而**没有足够的科学文章**来撰写您所追求质量的研究报告。

2:38

最好也查看一下其他步骤的输出。也许 LLM 找到了最好的五个来源，但您最终得到的却是“Astro Kid News”、“SpaceBot 2000”、“Space Fun News”等等。

2:45

正是通过查看这些**中间输出**，您可以**大致了解每一步输出的质量**。

2:52

为了引入一些术语，所有中间步骤输出的**整体集合**通常被称为该代理运行的**追踪 (trace)**。

3:00

您在其他资料中还会看到另一个术语，即**单个步骤的输出**有时被称为一个 **span**。

3:05

这是来自**计算机可观察性 (computer observability) 文献**中的术语，人们试图弄清楚计算机正在做什么。

3:11

在本课程中，我使用“追踪 (trace)”这个词较多，使用“span”较少，但您在互联网上可能会看到这两个术语。

3:16

通过**阅读追踪**，您可以开始**非正式地了解**哪个组件可能是**问题最大**的。

3:22

为了更系统地进行这项工作，将注意力集中在**系统表现不佳的案例**上会很有帮助。

3:27

也许有些文章写得很好，输出完全令人满意。我会把这些放在一边，**专注于**收集一组**研究代理的最终输出不完全令人满意的例子**。

3:33

这就是为什么我们称之为**错误分析**的原因之一，因为我们想要**专注于系统出错的案例**，并且我们要**追溯**，找出**哪个组件**对研究代理输出中的错误**负有最大的责任**。

3:42

为了使这更严谨，与其阅读并获得非正式的感觉，

3:48

您不妨**建立一个电子表格**，更明确地**统计错误发生的位置**。

3:54

我所说的“错误”，是指**某一步的输出**，在给定相似输入的情况下，**表现明显差于人类专家**所能给出的输出。

4:00

所以我通常会在电子表格中自己进行这项工作。我可能会建立一个这样的电子表格。

4:07

对于**第一个查询**，我查看“黑洞科学的最新发展”。我发现**搜索结果**有**太多博客文章、大众媒体文章，科学论文不够**。

4:13

然后基于此，确实\*\*“五佳来源”**选得**不好\*\*。但我在这里不会说“五佳来源”做得差，

4:18

因为如果 LLM 选择五佳来源的**输入**都是**不严谨**的文章，那么我不能怪它没有选出更好的文章，因为它已经尽力了，或者说在给定相同的选择范围时，它做得和任何人类一样好。

4:26

然后您可以对不同的提示进行这个过程。**“在西雅图租房与买房”**。也许它**遗漏**了一个知名的博客。

4:32

**“用于水果采摘的机器人技术”**。也许在这种情况下，我们查看后会说：“哦，**搜索词太笼统了**”，然后“**搜索结果**也**不好**”，等等。

4:36

然后，基于此，我会在我的电子表格中**统计**我观察到**不同组件出现错误的频率**。

4:45

因此，在这个例子中，我对**搜索词**不满意的情况占 **5%**，但我对**搜索结果**不满意的情况占 **45%**。

4:52

如果我真的看到了这种情况，我可能会仔细检查**搜索词**，以确保搜索词确实没问题，并且**不是搜索词的选择不当**导致了糟糕的搜索结果。

4:57

但如果我真的认为搜索词没问题，但**搜索结果**有问题，那么我就会仔细查看我使用的**网页****搜索引擎**，以及是否有任何**参数**可以调整，使其返回**更相关或更高质量的结果**。

5:04

正是这种类型的分析告诉我，在这个例子中，我可能真的应该将注意力集中在**修复搜索结果**上，而不是这个代理式工作流程的其他组件上。

5:12

因此，总结一下这个视频，我发现**养成查看追踪 (traces) 的习惯**很有用。

5:19

在您构建了一个代理式工作流程之后，请**查看中间输出**，以了解它**每一步实际在做什么**，这样您就能更好地理解不同步骤的表现是好是坏。

5:26

而一个**更系统的错误分析**（也许通过电子表格完成），可以让您**收集统计数据**或**统计哪个组件表现不佳的频率最高**。

5:32

因此，通过查看**哪些组件表现不佳**，以及我对**如何有效改进不同组件有想法**，

5:40

这将使您能够**确定应该改进哪个组件的优先级**。

5:45

也许某个组件有问题，但我**没有任何改进它的想法**，那么这可能意味着**不应该将其优先级定得很高**。

5:50

但如果有一个组件**产生了大量错误**，并且我**有改进它的想法**，那么这将是**优先处理该组件**的好理由。

5:55

我只想强调，**错误分析**对于您决定**将精力集中在哪里**是一个非常有用的输出，因为在任何复杂的系统中，您能做的事情实在太多了。

6:01

很容易选择一件事情去做，然后花费数周甚至数月的时间，**结果却发现**这对您系统的整体性能**并没有带来任何改善**。

6:09

因此，**利用错误分析来决定您的工作重点**，对于**提高您的效率**来说，被证明是**极其有用**的。

6:15

在这个视频中，我们以研究代理为例介绍了错误分析，但我认为错误分析是一个非常重要的话题，我想和您一起探讨更多额外的例子。

6:22

所以，让我们进入下一个视频，在那里我们将看到更多错误分析的例子。

3. ## **More error analysis examples** —— 更多错误分析示例

我发现对于许多开发人员来说，只有通过**看到多个例子**，才能获得练习并磨练出关于**如何进行错误分析**的直觉。

0:05

因此，让我们再看**两个例子**，我们将分别探讨**发票处理**和**回复客户邮件**。

0:09

这是我们用于**发票处理**的工作流程，我们有一个清晰的流程来遵循代理式工作流程：**识别四个必填字段**，然后将它们**记录到数据库**中。

0:16

在本单元第一个视频的例子中，我们提到该系统经常在**发票的到期日**上犯错。

0:28

因此，我们可以进行错误分析，尝试找出这可能是**哪个组件**造成的。

0:36

例如，是**PDF 转文本 (PDF to text)** 出了错，还是 **LLM 从 PDF 转文本组件的输出中提取了错误的日期**？

0:41

为了进行错误分析，我会尝试找到一些**提取数据不正确**的例子。

0:51

所以，和上一个视频一样，将重点放在**性能不达标**的例子上是很有用的，以便弄清楚这些例子到底出了什么问题。

0:58

忽略那些日期提取正确的例子，但尝试找到\*\* 10 到 100 张**日期提取**错误的\*\*发票。

1:05

然后我会仔细查看，试图弄清楚问题的原因是：**PDF 转文本**提取的日期或文本**有误**，以至于**即使是人类**也无法判断到期日；还是**PDF 转文本的输出看起来足够好**，但 **LLM** 在被要求提取日期时，**却拉取了错误的日期**，例如识别了**发票日期**而不是**发票的到期日**。

1:12

因此，您可能会建立一个这样的小型电子表格，查看 20 张发票，然后统计：**PDF 转文本**提取日期或文本**错误**的频率是多少，**LLM**** 数据提取**提取**错误日期**的频率又是多少。

1:25

在这个例子中，看起来 **LLM**** 数据提取**导致了**更多的错误**。

1:48

这告诉我，我或许应该将精力**集中在 ****LLM**** 数据提取组件**上，而不是 **PDF 转文本**上。

1:53

这一点很重要，因为如果没有这次错误分析，我可以想象有些团队会花费数周或数月尝试**调整 PDF 转文本**，结果却发现它对最终系统的性能**影响甚微**。

1:59

哦，顺便说一下，底部的这些百分比**加起来可能不是 100%**，因为这些错误**并非****互斥**。

2:14

来看**最后一个例子**，让我们回到**回复客户邮件**的代理式工作流程，

2:22

在这个流程中，LLM 在接收到像这样的客户邮件（询问订单）后，会**拉取订单详情**，**从数据库中获取信息**，然后**起草一份回复**供人工审核。

2:27

同样，我会找到一些**最终输出不令人满意**的例子，然后尝试找出**哪里出了问题**。

2:40

可能出错的地方包括：

2:46

* 也许 **LLM**** 编写了不正确的数据库查询**。因此，当查询发送到数据库时，它**未能成功拉取客户信息**。
* 也许**数据库数据损坏**。所以，即使 LLM 编写了一个完全合适的数据库查询（比如使用 SQL 或其他查询语言），数据库中也**没有正确的信息**。
* 也许在获得了**正确的客户订单信息**后，**LLM**** 编写的邮件**仍然**不太对劲**。

2:50

因此，我同样会查看一些**最终输出不令人满意**的邮件，并尝试找出**哪里出了问题**。

3:01

例如，在**邮件一**中，我们发现 **LLM 在查询中要求了错误的表**，它在创建数据库查询的方式上就要求了错误的数据。

3:10

在**邮件二**中，我可能发现**数据库实际上存在一个错误**。

3:13

而且，在给定的输入下，**LLM 也写了一封次优 (subalternate) 的邮件**，等等。

3:20

在这个例子中，在查看了许多邮件之后，我可能会发现**最常见的错误**在于 **LLM 编写数据库查询**（例如 SQL 查询）以**获取相关信息的方式**。

3:38

而**数据库**大多是**正确**的，尽管那里有一点数据错误。

3:44

**LLM 撰写邮件的方式**也存在一些错误，可能它有 **30% 的时间写得不太到位**。

3:50

这告诉我，**最值得**我去做的也许是**改进 ****LLM**** 编写查询的方式**。

3:58

第二重要的可能是**改进撰写最终邮件的提示**。

4:02

这样的分析可以告诉您，在所有**不完全正确**的问题中，**75%** 的问题来自**数据库查询**。

4:08

这对于指导您**将精力集中在哪里**是**非常有帮助**的信息。

4:14

当我开发代理式 AI 工作流程时，我经常会使用这种类型的错误分析来告诉我**接下来应该把注意力集中在什么工作上**。

4:20

当您做出这个决定后，事实证明，为了**补充**我们在本单元前面讨论的**端到端评估**，**不仅评估整个端到端系统**，**也评估单个组件**通常很有用，

4:36

因为这可以使您**更有效地改进**您决定重点关注的**单个组件**。

4:45

所以，让我们进入下一个视频，学习**组件级评估 (component-level evals)**。

4. ## **Component-level evaluations** —— 组件级评估

让我们来看看**如何构建和使用组件级评估 (component-level evals)**。

0:04

在我们的**研究代理**的例子中，我们说过研究代理**有时会遗漏关键点**。但如果问题出在**网页搜索**上，那么**每次**我们更换搜索引擎时，都需要**重新运行整个工作流程**，

0:09

这虽然可以提供一个良好的性能指标，但这种评估**成本很高**。

0:15

此外，这是一个**相当复杂的工作流程**，所以即使网页搜索有所改进，**其他组件的随机性引入的噪声**也可能会**使我们更难看到网页搜索质量的微小改进**。

0:26

因此，作为**只使用端到端评估的替代方案**，我可能会考虑构建一个**专门衡量网页搜索组件质量**的评估。

0:38

例如，为了衡量网页搜索结果的质量，您可以**创建一份黄金标准网页资源列表 (gold standard web resources)**。

0:43

因此，对于少数几个查询，请专家说出：“这些是**最具权威性的来源**，如果有人搜索互联网，他们**确实应该找到这些网页**，或者找到其中任何一个网页都是好的。”

0:53

然后，您可以编写代码来**捕获**网页搜索输出中有多少与**黄金标准网页资源**相对应。

1:03

信息检索领域的标准指标，例如 **F1 分数**（如果您不知道它是什么意思，不用担心细节），但存在**标准指标**，允许您衡量网页搜索返回的网页列表，与专家确定的黄金标准网页资源的**重叠程度**。

1:15

有了这个，您就拥有了一种**仅评估网页搜索组件质量**的方法。

1:29

因此，当您**更改**您关心的网页搜索的**参数或****超参数**时，

1:34

例如，当您**更换不同的网页****搜索引擎**时（可以尝试 Google、Bing、DuckDuckGo、Tivoli、U.com 等），或者当您**更改返回结果的数量**时，或者当您**更改要求搜索引擎搜索的日期范围**时，

1:40

这可以**非常快速地**让您判断**网页搜索组件的质量是否正在提高**，并实现**更多的增量改进**。

1:50

当然，在宣布工作完成之前，最好**运行一个端到端评估**，以确保在调整了一段时间网页搜索系统后，您**确实改进了整个系统的性能**。

2:01

但在**逐个调整这些****超参数**的过程中，您可以通过**仅评估一个组件**来**更高效地**进行，而不是每次都需要重新运行端到端评估。

2:10

因此，**组件级评估**可以为特定的错误提供**更清晰的信号**。它实际上能让您知道您是否正在改进**网页搜索组件**或您正在处理的**任何组件**，并**避免**整体端到端系统的复杂性带来的**噪声**。

2:20

如果您正在进行一个项目，其中**不同的团队**专注于**不同的组件**，那么让一个团队拥有**自己非常清晰的优化指标**，而**无需担心所有其他组件**，也会更有效率。

2:37

因此，这使得团队能够**更快地解决一个更小、更有针对性的问题**。

2:43

所以，当您决定着手改进某个组件时，请考虑是否**值得建立一个组件级别的评估**，以及这是否能让您**更快地提高该组件的性能**。

2:53

现在您可能想知道的一件事是，如果您决定改进一个组件，**您究竟如何着手让那个组件工作得更好呢**？

3:05

让我们在下一个视频中看一些这方面的例子。

5. ## **【实验】Ungraded Lab: Adding a component-level eval to the research ****workflow** —— 非评分实验：将组件级评估加入研究流程

### M4 智能体 AI - 向研究工作流添加一个组件级评估

1. #### 介绍

在之前的评分实验（M3）中，你构建了一个使用工具的研究智能体，它执行了三个步骤的工作流：

* 搜索网络信息。
* 对其输出进行反思（Reflect）。
* 发布一个清晰的 HTML 报告。

现在，在这个非评分实验中，你将专注于评估该工作流中的**一个组件**：**研究步骤**。

在这里，你将不再生成和精炼文章，而是设计一个**组件级评估**来检查研究步骤返回的**源**（sources）的质量。

该评估将把智能体检索到的 **URLs** 与**预定义的首选域列表**（例如，`arxiv.org`, `nature.com`, `nasa.gov`）进行比较。

这使你能够使用**客观的、基于每个例子的基本事实（ground truth）评估**，来量化系统是否正在从**可信赖的来源**中获取信息。

**1.1. 实验概述**

在视频中，Andrew 展示了一个网络搜索结果**质量较差**的案例，这使得人们难以相信检索到的信息。

基于这个例子，在本实验中，你将通过将来源与**预定义的首选域列表**进行比较来评估它们的可靠性。

对于此评估，我们将重点关注课程中强调的一个例子——主题\*\*“黑洞科学的最新进展”\*\*。

目的是验证网络搜索工具是否返回了来自首选域的来源，并量化首选来源与总结果的比例。

这个评估将作为一个单独的函数实现，执行一个**客观的、基于每个例子的检查**。它将：

* 解析 Tavily（我们的网络搜索工具）的输出。
* 识别哪些 URL 属于**首选域**列表。
* 计算首选来源与检索到的总来源的**比例**。
* 返回一个布尔标志（通过/失败，PASS/FAIL）和一个可以直接嵌入报告的 **Markdown**** 格式摘要**。

**1.2. 🎯 学习目标**

你将学会如何：

* 编写一个函数，用于检查网络搜索 API 的搜索结果中是否包含**首选来源**。
* 创建一个评估，以验证你的来源是否来自**首选域**。
* 向网络搜索功能添加一个**组件级评估**。

---

2. #### 设置：导入库和加载环境

与之前的实验一样，你首先导入所需的库并初始化你的环境。

```Plain
# =========================# Imports# =========================# --- Standard library from datetime import datetime
import json
import re
 
# --- Third-party ---from aisuite import Client
 
# --- Local / project ---import research_tools
import utils
 
client = Client()
```

---

1. 研究步骤 – `find_references`

在评分实验中，你实现的函数在一个步骤中完成了**网络搜索和撰写报告草稿**。

在这里，我们将网络搜索功能拆分到一个名为 `find_references` 的单独函数中。这使你可以独立于写作和反思步骤来评估搜索结果，因为本实验仅关注网络搜索步骤的输出，我们将省略后续步骤。

请注意与评分实验实现的两个关键区别：

* 这个新函数使用了 **AISuite**，它为你自动管理工具调用（而不是使用 OpenAI SDK 编写手动工具调用代码）。
* 该函数还告知 **LLM**** 当前日期**，这有助于提高时效性查询的相关性。

`find_references` 的作用是**从 ****Arxiv****、Tavily 和 ****Wikipedia**** 等工具中收集外部信息**。由于这些结果的质量直接影响评分实验的输出，因此这是你可以应用**评估方法**的阶段——例如，检查返回的 URL 是否来自你的**首选域**列表。

Python

```Plain
def find_references(task: str, model: str = "openai:gpt-4o", return_messages: bool = False):"""Perform a research task using external tools (arxiv, tavily, wikipedia)."""
 
    prompt = f"""
    You are a research function with access to:
    - arxiv_tool: academic papers
    - tavily_tool: general web search (return JSON when asked)
    - wikipedia_tool: encyclopedic summaries
 
    Task:
    {task}
 
    Today is {datetime.now().strftime('%Y-%m-%d')}.
    """.strip()
 
    messages = [{"role": "user", "content": prompt}]
    tools = [
        research_tools.arxiv_search_tool,
        research_tools.tavily_search_tool,
        research_tools.wikipedia_search_tool,
    ]
 
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice="auto",
            max_turns=5,
        )
        content = response.choices[0].message.content
        return (content, messages) if return_messages else content
    except Exception as e:
        return f"[Model Error: {e}]"# 运行以下单元格以测试研究功能。# 此任务将检索两篇关于黑洞科学最新进展的论文并显示结果。

research_task = "Find 2 recent papers about recent developments in black hole science"
research_result = find_references(research_task)
 
utils.print_html(
    research_result,
    title="Research Function Output"
)
```

---

2. 评估步骤 – 首选域

并非所有通过网络搜索检索到的来源都同样可靠。

在本实验中，我们专注于**上一个评分实验中的一个步骤**——`find_references` 研究步骤——并展示如何设计一个**组件级评估**，检查返回的域是否属于**预定义的首选域**列表。

这是一个**具有明确基于每个例子的基本事实的客观评估**的例子。回顾讲座中的两个评估维度：沿着这些维度，我们正在处理**左上象限**——应用于每个例子的**客观评估**和**明确定义的基本事实**。

为什么选择组件级评估？

正如 Andrew 在讲座中提到的：

* 如果问题出在网络搜索（通常是评分实验工作流中的**第一步**），每次重新运行**整个**流程（搜索 → 草稿 → 反思）可能会**耗费成本**且引入**噪音**。
* 网络搜索质量的小幅改进可能会被后续组件引入的随机性所掩盖。
* 通过**单独**评估网络搜索，你可以获得该组件是否正在改进的**更清晰信号**。

当多个团队致力于系统的不同部分时，组件级评估也很高效：每个团队都可以使用清晰的指标来优化自己的组件，而无需运行或等待完整的端到端测试。

##### 我们如何评估？

我们的评估是**客观的**，因此可以使用代码进行评估。它具有特定于例子的基本事实——针对此黑洞查询的首选来源列表。要构建评估，你将：

* 提取 Tavily 返回的 URL。
* 将它们与**预定义的首选域列表**（例如，`arxiv.org`, `nature.com`, `nasa.gov`）进行比较。
* 计算**首选结果与总结果的比例**。
* 返回一个 **PASS/FAIL 标志**以及一个 **Markdown**** 格式的摘要**。

这提供了一个可重现、低成本的指标，告诉我们研究组件（且仅此步骤）是否正在从可信赖的来源获取信息。

Python

```Plain
# list of preferred domains for Tavily results
TOP_DOMAINS = {
    # General reference / institutions / publishers"wikipedia.org", "nature.com", "science.org", "sciencemag.org", "cell.com",
    "mit.edu", "stanford.edu", "harvard.edu", "nasa.gov", "noaa.gov", "europa.eu",
 
    # CS/AI venues & indexes"arxiv.org", "acm.org", "ieee.org", "neurips.cc", "icml.cc", "openreview.net",
 
    # Other reputable outlets"elifesciences.org", "pnas.org", "jmlr.org", "springer.com", "sciencedirect.com",
 
    # Extra domains (case-specific additions)"pbs.org", "nova.edu", "nvcc.edu", "cccco.edu",
 
    # Well known programming sites"codecademy.com", "datacamp.com"
}
 
def evaluate_tavily_results(TOP_DOMAINS, raw: str, min_ratio=0.4):"""
    Evaluate whether plain-text research results mostly come from preferred domains.
 
    Args:
        TOP_DOMAINS (set[str]): Set of preferred domains (e.g., 'arxiv.org', 'nature.com').
        raw (str): Plain text or Markdown containing URLs.
        min_ratio (float): Minimum preferred ratio required to pass (e.g., 0.4 = 40%).
 
    Returns:
        tuple[bool, str]: (flag, markdown_report)
            flag -> True if PASS, False if FAIL
            markdown_report -> Markdown-formatted summary of the evaluation
    """# Extract URLs from the text
    url_pattern = re.compile(r'https?://[^\s\]\)>\}]+', flags=re.IGNORECASE)
    urls = url_pattern.findall(raw)
 
    if not urls:
        return False, """### Evaluation — Tavily Preferred Domains
No URLs detected in the provided text. 
Please include links in your research results.
"""# Count preferred vs total
    total = len(urls)
    preferred_count = 0
    details = []
 
    for url in urls:
        domain = url.split("/")[2]
        preferred = any(td in domain for td in TOP_DOMAINS)
        if preferred:
            preferred_count += 1
        details.append(f"- {url} → {'✅ PREFERRED' if preferred else '❌ NOT PREFERRED'}")
 
    ratio = preferred_count / total if total > 0 else 0.0
    flag = ratio >= min_ratio
 
    # Markdown report
    report = f"""
### Evaluation — Tavily Preferred Domains
- Total results: {total}
- Preferred results: {preferred_count}
- Ratio: {ratio:.2%}
- Threshold: {min_ratio:.0%}
- Status: {"✅ PASS" if flag else "❌ FAIL"}
 
**Details:**
{chr(10).join(details)}
"""return flag, report
```

**🔎 为什么这是一个客观评估：**

从 Tavily 检索到的每个 URL 都与\*\*预定义的首选域列表（TOP\_DOMAINS）\*\*进行比较：

* 如果域匹配 → ✅ 首选（PREFERRED）
* 否则 → ❌ 非首选（NOT PREFERRED）

这会根据首选来源的比例是否超过给定阈值，产生一个清晰的 **PASS/FAIL** 信号。因为基本事实（首选与非首选）是为每个例子**明确定义**的，所以评估是**客观的**且**可重现**的。

Python

```Plain
# 运行单元格以显示首选域样本、研究结果和评估摘要（通过/失败及详情）。

utils.print_html(json.dumps(list(TOP_DOMAINS)[:4], indent=2), title="Sample Trusted Domains")
 
utils.print_html("<h3>Research Results</h3>" + research_result, title="Research Results")
 
flag, report = evaluate_tavily_results(TOP_DOMAINS, research_result)
utils.print_html("<pre>" + report + "</pre>", title="<h3>Evaluation Summary</h3>")
```

##### 亲手尝试！

现在轮到你了。

在本节中，你可以直接对**研究步骤**及其**评估**进行实验：

* **主题：** 选择不同的研究主题。
* **首选域：** 编辑或扩展 `TOP_DOMAINS` 列表。
* **评估比例：** 调整 `min_ratio`（例如，0.4 = 至少 40% 的首选来源）。

进行编辑后，重新运行下面的单元格，查看评估如何变化。

```Plain
# === 5.1. Try it yourself: topic, ratio & preferred domains ===# Edit these parameters before running the cell
 
topic = "recent developments in black hole science"   # <- Change the topic here
min_ratio = 0.4                                       # <- Change threshold (0.0–1.0)
run_reflection = True                                 # <- Set False to skip Step 4# Short list of preferred domains (edit or expand as needed)
TOP_DOMAINS = {
    "wikipedia.org", "nature.com", "science.org", "arxiv.org",
    "nasa.gov", "mit.edu", "stanford.edu", "harvard.edu"
}
 
# Show a sample of preferred domainsimport json
utils.print_html(
    json.dumps(sorted(list(TOP_DOMAINS)), indent=2),
    title="<h3>Sample Preferred Domains</h3>"
)
 
# 1) Research
research_task = f"Find 2–3 key papers and reliable overviews about {topic}."
research_output = find_references(research_task)
utils.print_html(research_output, title=f"<h3>Research Results on {topic}</h3>")
 
# 2) Evaluate sources (preferred domains ratio)
flag, eval_md = evaluate_tavily_results(TOP_DOMAINS, research_output, min_ratio=min_ratio)
utils.print_html("<pre>" + eval_md + "</pre>", title="<h3>Evaluation Summary</h3>")
```

---

3. #### 总结

你刚刚看到了如何评估**一个组件**的性能：`find_references` 研究步骤。

你的组件级评估检查了检索到的 URL 是否在预定义的**首选域**列表中。

这是一个**客观评估**的例子，具有清晰的**基于每个例子的基本事实**。

为了构建一个评估集，你可以设计大约 10 个涵盖不同主题（天文学、机器人学、金融等）的提示，并为每个主题定义首选域。

检索到的来源中与首选域列表匹配的百分比提供了一个有用的**指标**来指导改进，例如调整提示或工具参数。

这种方法比通过反思和重写来评估完整的文章**更简单、更便宜**，因为你只关注网络搜索组件。

🎉 **恭喜！**

你设计了一个**组件级评估**，使你的研究智能体更加可靠。

通过直接检查来源的质量，你引入了一个**客观、可重现且经济高效**的保障措施。

这与 Andrew 讲座中强调的观点一致：**组件级评估**让你无需评估整个流程的开销，即可测试 AI 系统的各个部分。

6. ## **How to address problems you identify** —— 如何解决已发现的问题

0:06

一个代理式工作流程可能包含许多**不同类型的组件**，因此您**改进不同组件的工具**也会有很大的不同。但我想和大家分享一些我看到的一般模式。

0:09

您的代理式工作流程中的某些组件将是**非 LLM 基础**的，

0:15

例如**网页****搜索引擎**，或者一个**文本检索组件**（如果那是您的 RAG 或检索增强生成系统的一部分），**代码执行**的组件，

0:20

或者是一个**单独训练的机器学习模型**，例如用于**语音识别**或**检测图片中的人物**等等。

0:24

所以，这些**非 LLM 基础的组件**通常会有一些您可以**调整的参数或****超参数**。

0:34

例如，对于**网页搜索**，您可以调整**结果的数量**，或者您要求搜索引擎考虑的**日期范围**。

0:39

对于 **RAG 文本检索组件**，您可以更改决定它认为哪些文本相似的**相似度阈值**，或者**块大小 (chunk size)**。RAG 系统通常会将文本切分成更小的块进行匹配，因此这是您可以使用的主要超参数。

0:44

或者对于**人物检测**，您可以更改**检测阈值**，即它的敏感度以及声明“发现一个人”的可能性，这将权衡**误报**和**漏报**。

0:55

如果您没有完全理解我刚才讨论的超参数的所有细节，请不用担心。细节不那么重要，但通常这些组件都有您可以调整的**参数**。

1:00

当然，您也可以尝试**替换**该组件。我在我的代理式工作流程中经常这样做，我会**替换不同的 RAG ****搜索引擎**或**替换不同的 RAG 提供商**等等，只是为了看看是否有其他提供商可能工作得更好。

1:12

由于非 LLM 基础组件的多样性，我认为改进它们的技术会**更加多样化**，并取决于该组件**具体在做什么**。

1:29

对于**基于 LLM 的组件**，您可以考虑以下一些选项：

1:37

一个是尝试**改进您的提示 (prompts)**。您可以尝试**添加更明确的****指令**。或者，如果您了解什么是**少样本提示 (few-shot prompting)**，它指的是添加**一个或多个具体的输入示例和期望输出示例**。

1:43

因此，少样本提示（您也可以从一些深度学习短期课程中了解）是一种可以**为您的 ****LLM**** 提供一些示例**，从而有望帮助它**写出性能更好的输出**的技术。

1:55

或者您可以尝试**不同的 ****LLM**。使用 AI Suite 或其他工具，尝试**多个 LLM**可能会非常容易，然后您可以使用**评估**来为您的应用程序**选择最佳模型**。

2:06

有时，如果**单个步骤对于一个 ****LLM**** 来说过于复杂**，您可以考虑是否要将任务**分解成更小的步骤**。或者分解成一个**生成步骤**和一个**反思步骤**。

2:19

但更一般地说，如果您在一个步骤中包含了**非常复杂的****指令**，单个 LLM 可能难以遵循所有这些指令。您可以将任务**分解**成更小的步骤，这样也许通过**连续调用两到三次**会更容易准确地执行。

2:30

最后，当其他方法效果不够好时，可以考虑**微调模型 (fine-tuning a model)**。

2:45

这通常比其他选项**复杂得多**，因此在**开发人员时间成本**上也可能**昂贵得多**。但如果您有一些可以用来微调 LLM 的数据，这可能会为您带来比**单独提示**更好的性能。

2:51

所以我通常不会微调模型，除非我已经**穷尽了其他选项**，因为微调往往非常复杂。但是对于某些应用程序，如果在尝试了所有其他方法之后，性能仍停留在**例如 90% 或 95%**，而我真的需要**挤出最后那几个百分点的改进**，那么有时**微调我自己的定制模型**是一个很好的技术。

3:06

由于成本高昂，我倾向于只在**更成熟的应用程序**上进行此操作。

3:19

事实证明，当您尝试**选择一个 ****LLM** 来使用时，对于您作为开发人员来说，非常有帮助的一件事是，您对**不同大型语言模型的智能程度或能力如何**有良好的直觉。

3:25

您可以做的一件事是**尝试很多模型**，看看哪个效果最好。但我发现，随着我与不同模型的合作，我开始**磨练出关于哪些模型最适合哪种类型任务的直觉**。当您磨练出这些直觉时，您在为模型**编写好的提示**以及为任务**选择好的模型**时也会更有效率。

3:32

所以我想和您分享一些关于**如何磨练**您的直觉，以判断**哪些模型**将适用于您的应用程序的想法。

3:41

让我们用一个例子来说明：使用 LLM 遵循指令**移除或编辑 ****PII****（个人身份信息）**。

3:47

所以您现在需要移除私人敏感信息。例如，如果您正在使用 LLM 总结客户电话，那么某个摘要可能是：“2023 年 7 月 14 日，Jessica Alvarez，社会安全号码是 [某个号码]，地址是 [某个地址]，业务支持工单 [某个工单]”等等。

3:56

因此，这段文本包含**许多敏感的个人身份信息**。

4:07

现在，假设我们想要**从这些摘要中移除所有 ****PII**，因为我们希望使用这些数据进行**下游的统计分析**，了解客户主要来电咨询什么。为了保护客户信息，我们在进行下游统计分析之前，想要**剥离 PII**。

4:16

因此，您可能会向 LLM 提示指令，要求它**识别**下方文本中**所有 ****PII**** 的情况**，然后返回**已编辑的文本**，格式为“已编辑: [文本]”等等。

4:21

事实证明，**较大的前沿模型**在**遵循****指令**方面往往**好得多**，而**较小的模型**在回答简单的**事实性问题**方面可能相当不错，但在**遵循指令**方面就是**不如**它们。

4:36

如果您在较小的模型上运行这个提示，例如 OpenWay Llama 3.1（80 亿参数），它可能会生成像这样的输出。它说“已识别的 PII 是社会安全号码和地址”，然后它编辑如下，等等。但它**实际上犯了一些错误**。它没有正确遵循指令。它显示了列表，然后编辑了文本，然后又返回了另一个列表，而这本不应该出现。在这个 PII 列表中，它**遗漏了名字**。然后我认为它**也没有完全编辑地址**。

4:50

所以细节不重要，但它**没有完美地遵循这些****指令**，而且可能**遗漏了一点 ****PII**。

5:05

相比之下，如果您使用一个**更智能的模型**，一个**更擅长遵循****指令****的模型**，您可能会得到一个更好的结果，像这样，它**正确地列出了所有 PII** 并**正确地编辑了所有 PII**。

5:12

因此我发现，随着不同的 LLM 提供商**专注于不同的任务**，**不同的模型确实更适合不同的任务**。有些擅长编码，有些擅长遵循指令，有些擅长某些小众类型的事实。如果您能**掌握直觉**，知道哪些模型智能程度更高或更低，以及它们**更擅长或不擅长遵循哪种类型的指令**，那么您就能对使用哪些模型做出更好的决定。

5:30

所以，分享几个关于**如何做到这一点**的技巧：我鼓励您**经常试玩不同的模型**。

5:37

因此，每当有新的模型发布时，我通常会去尝试它，并在上面尝试不同的查询，包括**闭源专有模型**和**开源模型**。

5:41

我发现有时拥有**一套个人评估**也可能会有帮助，您有一套会向许多不同模型提出的问题，这可能有助于您**校准**它们在不同类型任务上的表现。

5:50

我经常做的另一件事，我希望对您也有用，那就是我花了很多时间**阅读其他人的提示**。

5:56

有时人们会在互联网上**发布他们的提示**，我经常会去阅读它们，以了解**提示的****最佳实践**是什么样子。或者我经常会和我认识的**各个公司的朋友**聊天，包括一些前沿模型公司，和他们分享我的提示，看看**他们是如何编写提示**的。

6:04

有时我还会去**下载我非常尊敬的人编写的开源软件包**，并深入研究该开源软件包，**找到作者编写的提示**，以便阅读，从而**磨练我关于如何编写良好提示的直觉**。

6:15

这是我鼓励您考虑的一种技术，**通过阅读许多其他人的提示**，这将帮助您**更好地自己编写提示**。我确实经常这样做，我也鼓励您这样做。

6:22

这将**磨练您的直觉**，了解模型擅长遵循哪种类型的指令，以及何时对不同的模型说某些话。

6:30

除了试玩模型和阅读他人的提示之外，如果您在您的代理式工作流程中**尝试各种不同的模型**，这也让您能够**磨练您的直觉**。

6:36

您会看到**哪些模型最适合哪种类型的任务**，无论是通过**查看追踪**获得非正式感觉，还是通过**查看组件级或端到端评估**，都可以帮助您评估不同模型在工作流程的不同部分表现如何。

6:45

然后您开始磨练直觉，不仅关于性能，还可能关于**使用不同模型的成本和速度的权衡**。

6:50

我倾向于使用 AI Suite 来开发我的代理式工作流程的原因之一是，因为它使**快速替换和尝试不同的模型**变得容易。这使得我在**尝试和评估**哪个模型最适合我的工作流程方面更有效率。

6:56

我们已经讨论了很多关于**如何提高不同组件的性能**，以期提高您端到端系统的整体性能。

7:07

除了提高输出的质量之外，您可能还想在您的工作流程中做的另一件事是**优化延迟 (latency) 和成本 (cost)**。

7:15

我发现对于很多团队来说，在开始开发时，通常**首先要担心的是输出的质量是否足够高**。但当系统运行良好并投入生产时，通常会有价值去让它**运行得更快**，同时**运行成本也更低**。

7:21

因此，在下一个视频中，让我们看看一些**关于改进代理式工作流程的成本和延迟**的想法。

7. ## **Latency, cost ****optimization** —— 延迟与成本优化

在构建代理式工作流程时，我通常会建议团队**专注于获得高质量的输出**，而将**优化成本和延迟**放在**之后**。

0:06

这并不是说成本和延迟不重要，但我认为**让性能或输出质量达到高水平**通常是**最困难的部分**，只有当它**真正运行良好**时，才应该关注其他事情。

0:11

有几次发生在我身上的事情是：我的团队构建了一个代理式工作流程，我们将其发布给用户，然后我们很幸运地**有如此多的用户使用它**，以至于**成本**实际上成为了一个问题，然后我们不得不**手忙脚乱地将成本降下来**。但这是一个**好的问题**，所以我倾向于通常较少担心成本。

0:22

这并不是说我完全忽略它，只是它在我担心的事项清单中**排得比较靠后**，所以除非我们有**非常多的用户**，以至于我们真的需要**降低每位用户的成本**。

0:36

至于**延迟**，我确实会有点担心，但同样，**不如确保输出质量高**那样重要。

0:47

但是，当您确实达到了需要优化成本和延迟的阶段时，拥有相应的工具将非常有用。让我们来看看一些**如何实现这一目标**的想法。

1:03

如果您想**优化代理式工作流程的延迟**，我通常会做的一件事是**对工作流程进行****基准测试****或计时**。

1:06

例如，在这个研究代理中，它需要**多个步骤**。如果我给**每个步骤**计时，可能会发现：LLM 生成搜索词需要 **7 秒**。网页搜索需要 **5 秒**，这一步需要 **3 秒**，另一步需要 **11 秒**，然后撰写最终文章平均需要 **18 秒**。

1:13

正是通过观察这个**总时间线**，我可以大致了解**哪些组件有最大的提速空间**。

1:20

在这个例子中，您可能有多种尝试。如果您**尚未利用****并行****处理**来处理某些步骤（例如网页抓取），那么考虑**并行执行其中一些操作**可能是值得的。

1:35

或者，如果您发现某些 LLM 步骤耗时过长（例如第一步需要 7 秒，最后一步 LLM 需要 18 秒），我可能也会考虑尝试一个**更小、可能稍微不那么智能的模型**，看看它是否仍然能很好地完成任务，或者我是否能找到一个**更快的 LLM 提供商**。

1:43

网上有很多针对不同 LLM 接口的 API，有些公司拥有**专门的硬件**，可以让他们**更快地**提供某些 LLM 服务。因此，有时尝试不同的 LLM 提供商是值得的，看看**哪些能最快地返回 token**。

1:57

但至少进行这种**计时分析**可以为您提供一个方向，告诉您应该将重点放在**哪些组件**上来**减少延迟**。

2:04

至于**优化成本**，类似的计算，即**计算每一步的成本**，也可以让您进行基准测试并决定**应该重点关注哪些步骤**。

2:13

许多 LLM 提供商根据**输入和输出的长度按 token 收费**。许多 API 提供商**按 API 调用次数收费**。而计算步骤可能根据您**支付服务器容量的方式**以及**服务本身的成本**而有不同的费用。

2:20

因此，对于这样一个流程，您可能会决定，在这个例子中：这个 LLM 步骤的 token 平均成本为 **0.04 美分**，每次网页搜索 API 调用可能成本为 **1.6 美分**，token 成本这么多，API 调用成本这么多，PDF 转文本成本这么多，最终文章生成的 token 成本这么多。

2:34

这可能会再次让您大致了解**是否有更便宜的组件或更便宜的 LLM 可用**，从而看出**优化成本的最大机会在哪里**。

2:43

我发现这些**基准测试****练习**可以**非常清晰明了**，有时它们会清楚地告诉我，**某些组件根本不值得担心**，因为它们对成本或延迟的**贡献并不大**。

2:51

因此，我发现当**成本或延迟**成为问题时，仅仅通过**测量每一步的成本和/或延迟**，就通常能为您提供一个**决定将精力集中在优化哪些组件**的基础。

3:06

我们这个单元快要结束了。我知道我们涵盖了很多内容，但感谢您坚持下来。让我们继续本单元的**最后一个视频**进行总结。

8. ## **Development process summary** —— 开发流程总结

我们介绍了许多用于推动**严谨、高效**地构建代理式 AI 系统的技巧。

0:06

最后，我想和大家分享一下**经历这个过程的感觉**。

0:11

在我构建这些工作流程时，我感觉自己经常花时间在**两大主要活动**上：

0:16

一是**构建 (Building)**，即**编写软件**，尝试编写代码来改进我的系统。

0:21

二是**分析 (Analyzing)**，这有时感觉不像是在取得进展，但我认为它**同样重要**，它能帮助我决定**下一步应该将构建的精力集中在哪里**。

0:27

我经常在**构建**和**分析**之间来回切换，其中分析包括**错误分析**等。

0:32

举例来说，当构建一个新的代理式工作流程时，我通常会先**快速构建一个端到端系统**，甚至可能是一个**快速而粗糙的实现**。

0:38

这能让我开始**检查端到端系统的最终输出**，或者**通读追踪 (traces)**，以了解它**哪里做得好，哪里做得不好**。

0:43

仅仅通过查看追踪，有时就能给我一个**直觉 (gut sense)**，告诉我可能想要改进**哪些单个组件**。

0:54

因此，我可能会去**调整一些单个组件**，或者**继续调整整体的端到端系统**。

1:05

随着我的系统开始**变得更成熟**一些，那么除了手动检查少量输出和通读追踪之外，我可能会开始**构建评估 (evals)**，并用一个**小型数据集**（可能只有 10-20 个示例）来计算指标，至少是**端到端性能**的指标。

1:11

这进一步帮助我对**如何改进端到端系统**或**如何改进单个组件**有**更精细的视角**。

1:21

当系统**进一步成熟**时，我的分析可能会变得**更加严谨**，我开始进行**错误分析**，查看组件，并尝试**统计**单个组件导致**次优输出的频率**。

1:32

这种**更严格的分析**使我在决定下一步要处理**哪个组件**时能够**更加专注**，或为改进整个端到端系统**激发想法**。

1:49

最终，当系统**更加成熟**时，为了在**组件级别**推动**更高效的改进**，那时我可能还会构建**组件级评估**。

1:54

因此，构建代理式系统的工作流程通常是**来回反复**的。它**不是一个线性过程**。我们有时会**调整端到端系统**，然后做一些**错误分析**，接着**改进一个组件**，然后**调整组件级评估**。我倾向于在这**两种类型的技术**之间来回跳跃。

2:06

我观察到**经验不足的团队**通常会花大量时间在**构建**上，而花在**分析**上的时间要少得多，包括错误分析、构建评估等。这并非理想做法，因为正是这种**分析**能帮助你**真正集中精力**，决定**在哪里投入构建的时间**。

2:20

还有一个小技巧。目前市面上有**相当多**的工具可以帮助**监控追踪、记录运行时、计算成本**等等。这些工具很有帮助。我有时会使用其中一些，而且 DeepLearning.ai 的许多短期课程合作伙伴都提供这些工具，它们确实运行良好。

2:38

我发现，对于我最终参与的代理式工作流程，**大多数代理式工作流程都是相当定制化的**。因此，我最终会**自己构建相当定制化的评估**，因为我想捕捉到我的系统**不正确运行**的地方。

2:49

所以，尽管我确实使用了一些工具，但我最终也构建了许多**非常适合我特定应用及其所见问题**的**定制评估**。

3:00

感谢您坚持到现在，完成了五个单元中的第四个。

3:11

如果您能实施本单元中**哪怕一小部分**的想法，我认为您在实施代理式工作流程的**复杂程度**上，将**远远领先于绝大多数开发人员**。

3:24

希望您觉得这些材料有用，我期待在**最后一个单元**与您见面。我们将讨论一些用于构建**高度自主代理**的**更高级设计模式**。我们将在本课程的最后一个单元见。

9. ## **【付费，略过】Module 4 quiz** —— 模块4测验

# 课程内容总结

1. ## **Evaluations (evals)** —— 评估（evals）

### 构建代理式人工智能工作流程的实用技巧

本单元的核心主题是分享**构建代理式人工智能工作流程 (agentic ****AI**** workflows)** 的实用技巧，旨在帮助开发者更有效地构建此类系统。

1. 快速原型和迭代是关键

* **问题所在：** 在开发代理式 AI 系统时，很难事先知道系统将在哪里成功或失败，因此难以确定工作重点。
* **核心建议：** 采用\*\*“快速而粗糙 (quick and dirty)”\*\*的迭代方法。
  * **先构建**一个初始的、负责任的原型系统。
  * **试运行**并**观察输出**，找出表现不佳的地方。
  * 利用观察结果来**确定后续开发工作的优先级**和**方向**，避免长时间空泛的理论化。

2. 通过观察输出来发现错误模式并设置评估 (Evals)

演讲者通过两个具体案例演示了如何利用观察到的错误来驱动评估的创建和系统的改进。

##### 案例一：发票处理工作流程（提取到期日）

![](https://ucn3l6pxxkfq.feishu.cn/space/api/box/stream/download/asynccode/?code=YzJiYmQ4NzY0NWUxMWZmZDA5YWM0OTMyYWJkYzdkZTRfT2RXMkxrMzZ5eGJxaUdhYlNKZmtvQm1JZmJCbmwxaFJfVG9rZW46UjhzUWJ3dEtGb29lcDR4SFdIb2N1Vjhubk9lXzE3NjIyNjE0ODE6MTc2MjI2NTA4MV9WNA)

1. **观察发现错误：**
   1. 系统任务：从发票中提取四个必填字段并保存，特别是**到期日**（用于及时付款）。
   2. 通过检查 10-20 张发票的输出，发现一个常见的**错误模式**是系统**混淆了发票的开具日期和到期日**。

![](https://ucn3l6pxxkfq.feishu.cn/space/api/box/stream/download/asynccode/?code=NDVkY2FiMzVmNTZkNTYwMGVhZmM1OGYyMDk1Y2VmZWFfWGZ5WjZYeXZiajY3YXVSUU5mVnMwOXNIcVFMM1dha2tfVG9rZW46U3RZWmJXaFFNb29sY0t4eHZWOGN2SHpMbmdoXzE3NjIyNjE0ODE6MTc2MjI2NTA4MV9WNA)

2. **制定改进方向和评估：**
   1. 结论：系统在处理日期方面存在困难。
   2. 行动：改进系统以更好地提取到期日，并**编写一个评估 (eval)** 来**衡量日期提取的准确性**。
3. **构建评估（Eval）：**
   1. **测试集：** 找到 10-20 张发票，**手动**写下每个发票的正确到期日（即**基本事实/Ground Truth**）。
   2. **标准化格式：** 提示 LLM 始终以固定的年-月-日格式输出到期日，便于代码自动检查。
   3. **评估方式：** 编写代码（如正则表达式）提取日期，然后测试**提取出的日期是否等于基本事实日期**。
   4. **用途：** 利用这个指标来跟踪调整提示或其他系统组件后的**准确率提升**。
4. **总结改进流程：****构建系统 - 查看输出 - 发现错误 - 针对重要错误建立小型评估 - 调整系统以提高评估指标。**

##### 案例二：营销文案助理（限制字数）

1. **观察发现错误：**
   1. 系统任务：为 Instagram 图片生成标题，**要求标题最多 10 个词**。
   2. 通过观察输出，发现生成的文案内容大部分可以，但**经常超过 10 个词的长度限制**（例如 17 词、14 词、11 词）。
2. **构建评估（Eval）：**
   1. **测试集：** 准备 10-20 个测试任务（如太阳镜、咖啡机图片和提示）。
   2. **评估方式：** 编写代码**计算输出的词数**。
   3. **判断标准：** 将生成的文本长度与 **10 个词的目标限制**进行比较。
   4. **与案例一的区别：** 这个评估**没有每个例子的基本事实**，因为目标（10 个词）对所有例子都是一样的。

##### 案例三：研究代理（捕捉重要观点）

1. **观察发现错误：**
   1. 系统任务：根据提示撰写研究文章（如黑洞、机器人采摘）。
   2. 通过检查输出，发现文章有时会**遗漏**人类专家撰稿人会捕捉到的**高知名度或重要观点**。
2. **构建评估（Eval）：**
   1. **测试集与基本事实：** 针对每个提示，人工准备 **3 到 5 个黄金标准讨论点 (Gold Standard Discussion Points)** 作为**每个例子的基本事实**。
   2. **评估方式：** 由于提及这些观点的方式多种多样，简单的代码匹配不可行，因此使用 **LLM 作为评判者 (LLM-as-a-judge)**。
   3. **LLM 提示：** 要求评判 LLM 统计文章中提到了多少个黄金标准点，并返回得分和解释。
3. 评估的两个维度和四种类型

评估方式可以从两个维度划分，形成一个 2x2 的矩阵，用于指导评估的设计：

| 评估维度                                           | 客观评估 (Objective Evals) （用代码检查）                                   | 主观评估 (Subjective Evals) （用 LLM 作为评判者）                          |
| -------------------------------------------------- | --------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| 有每个例子的基本事实 (Per-Example Ground Truth)    | 案例一：发票日期提取 (每个发票有不同的正确日期，用代码检查是否匹配)         | 案例三：统计黄金标准点 (每个主题有不同的重要观点，用 LLM 检查是否充分提及) |
| 无每个例子的基本事实 (No Per-Example Ground Truth) | 案例二：营销文案长度 (所有标题都要求是 10 个词，用代码检查是否符合统一标准) | 评分标准评估 (Rubric Grading) (例如，根据统一的清晰度评分标准来评估图表)   |

4. 设计端到端评估的最终技巧
   
   1. **从“快速而粗糙”的评估开始：** 不要因为觉得评估是一个大型项目而瘫痪。先用 10-20 个例子开始，快速获得一些指标来**辅助人工观察**。
   2. **迭代改进评估：**
      1. 随着系统和评估的成熟，可以**增加评估集的规模**。
      2. 如果系统改进了但评估分数没有提高，这通常是**改进评估本身**的机会，使其更好地反映您对系统性能的判断。
   3. **以“专家人类”为灵感：** 对于自动化人类任务的系统，观察系统在哪些方面**性能不如人类专家**，以此作为下一阶段工作的重点。
5. ## **Error analysis and prioritizing next steps** —— 错误分析与后续步骤优先级

### 代理式 AI 工作流程的错误分析 (Error Analysis)

本段演讲的核心观点是：**在复杂的代理式 AI 工作流程中，通过系统性的错误分析来确定工作重点，是提高系统改进效率的关键。**

1. 为什么需要错误分析？

* **系统改进效率问题：** 代理式工作流程由多个组件（或步骤）构成。改进某些组件可能比改进其他组件**更有成效**。
* **直觉的风险：** 仅凭**直觉 (by gut)** 选择要改进的组件，可能导致浪费大量时间，但对系统整体性能提升很小。
* **效率的决定因素：** 团队的效率高低，很大程度上取决于他们能否进行**严谨的错误分析流程**，以指导工作重点。

2. 如何进行错误分析？

错误分析的核心是**观察和量化**，以找出工作流程中**表现最差**的组件。

**A. 检查追踪 (Traces) 和中间输出**

* **定义：** 代理在运行过程中，**每一步产生的中间输出**的整体集合被称为**追踪 (trace)**。单个步骤的输出有时被称为 **span**。
* **方法：****查看追踪**，观察每个步骤的输出质量，从而**非正式地了解**哪个组件可能存在问题。
* **示例（研究代理）：**
  * **步骤 1：生成搜索词** - 请人类专家判断搜索词是否合理。如果合理，则该组件可能无责。
  * **步骤 2：网页搜索结果** - 检查返回的 URL 和文章质量。如果返回了**太多非科学的博客或大众媒体文章**，则该组件是问题所在。
  * **步骤 3：选择五佳来源** - 如果输入给 LLM 的都是低质量文章，那么不能怪 LLM 没有选出更好的，问题仍在搜索结果。

**B. 聚焦错误案例并量化**

* **聚焦错误：** 将精力集中在**最终输出不令人满意**的少数案例上，而不是那些运行良好的案例。
* **建立电子表格进行量化：** 为了更严谨地分析，可以建立一个电子表格，**明确地统计**每个组件出现“错误”的频率。
  * **错误的定义：** 某一步的输出**明显差于**一个人类专家在给定相同输入时会给出的结果。
  * **统计示例：** 记录在所有被分析的错误案例中，**搜索词**不佳的比例、**搜索结果**不佳的比例等。
  * **指导决策：** 如果发现对**搜索结果**不满意的频率远高于对**搜索词**不满意的频率（如 45% vs 5%），那么工作重点就应放在**改进网页搜索引擎或调整其参数**上，而不是更改搜索词的生成逻辑。

3. 最终的工作优先级原则

错误分析的结果需要与改进的可能性结合起来，以确定最终的优先级：

1. **高错误率 + 有改进想法** \$\\rightarrow\$ **高优先级**：这个组件既是主要问题，也有明确的改进方向。
2. **高错误率 + 无改进想法** \$\\rightarrow\$ **低优先级**：虽然问题严重，但暂无高效解决方案，不宜浪费时间。

总之，**错误分析**是指导复杂系统开发工作重点的**关键输出**，它能帮助开发者避免徒劳无功的努力，从而**大大提高系统改进的效率**。

3. ## **More error analysis examples** —— 更多错误分析示例

好的，下面是对这段演讲内容的详细总结：

---

### 代理式 AI 工作流程的错误分析（进阶案例）

这段演讲通过两个额外的具体案例（发票处理和回复客户邮件），进一步演示了如何运用**系统性的错误分析 (Error Analysis)** 来精准定位代理式 AI 工作流程中**最需要改进的组件**。

1. 错误分析的实践基础

* **目的：** 帮助开发者磨练直觉，确定复杂的代理式工作流程中**哪个组件的改进最有价值**，避免将时间浪费在对最终性能影响小的组件上。
* **方法：****聚焦于错误案例**（性能不达标的例子），通过检查**中间输出 (Traces)**，用量化或统计的方式，找出错误**最常发生**的环节。

2. 案例一：发票处理（提取到期日）

**工作流程：** PDF \$\\rightarrow\$ (PDF 转文本) \$\\rightarrow\$ (LLM 数据提取) \$\\rightarrow\$ 数据库记录。

**发现的问题：** 提取的**到期日**经常出错。

**错误分析过程：**

1. **收集错误样本：** 忽略正确的发票，收集 10 到 100 张到期日提取**错误**的发票。
2. **定位错误来源：**
   
   1. **PDF 转文本错误：** 文本提取太差，导致连人类都无法识别到期日。
   2. **LLM 数据提取错误：** PDF 转文本的输出足够好，但 LLM 却错误地拉取了**其他日期**（如发票日期而非到期日）。
3. **结果指导：** 假设统计发现 **LLM 数据提取**导致了**更多的错误**（例如，占错误总数的大部分）。
   
   1. **结论：** 应将精力集中在**改进 LLM 数据提取组件**上（如优化提示），而不是花费数周时间徒劳地改进 PDF 转文本组件。
4. 案例二：回复客户邮件

**工作流程：** 客户邮件 \$\\rightarrow\$ (LLM 编写数据库查询) \$\\rightarrow\$ 数据库 \$\\rightarrow\$ (LLM 起草回复) \$\\rightarrow\$ 人工审核。

**发现的问题：** 最终邮件回复不令人满意。

**错误分析过程：**

1. **收集错误样本：** 收集最终回复不令人满意的客户邮件。
2. **定位错误来源：**
   
   1. **LLM 查询编写错误：** LLM 编写了错误的 SQL 查询，导致无法获取客户信息。
   2. **数据库数据错误：** 数据库数据损坏或不正确，即使查询正确也无法获取正确信息。
   3. **LLM 邮件撰写错误：** 即使获取了正确信息，LLM 撰写的邮件内容或语气不妥。
3. **结果指导：** 假设统计发现，**LLM 编写数据库查询**是**最常见的错误**（例如，占所有问题的 75%），而 LLM 撰写邮件的错误相对较少（例如 30%）。
   
   1. **优先级：**
      * **首要重点：** 改进 **LLM 编写查询**的方式。
      * **次要重点：** 改进**撰写最终邮件的提示**。
4. 结论：从错误分析到下一步行动

* **核心价值：** 错误分析能提供**量化信息**，以决定**接下来将精力集中在哪里**。例如，如果 75% 的问题来自数据库查询，那么解决查询问题将带来最大的效率提升。
* **引出后续：** 一旦通过错误分析确定了要改进的**单个组件**，下一步就是引入**组件级评估 (Component-Level Evals)** 来更高效地推动该组件的改进。

4. ## **Component-level evaluations** —— 组件级评估

### 组件级评估 (Component-Level Evals)

这段演讲介绍了在代理式 AI 工作流程中，除了端到端评估 (End-to-End Evals) 之外，使用**组件级评估 (Component-Level Evals)** 来提高开发效率和针对性改进的方法。

1. 组件级评估的必要性

* **端到端评估的局限性：**
  * **成本高昂：** 每进行一次小改动（如更换网页搜索引擎），都需要重新运行整个复杂的工作流程进行端到端评估，成本很高。
  * **信号不清：** 整个工作流程中其他组件的**随机性或噪声**可能会掩盖被改进组件（如网页搜索）带来的微小、增量改进。
* **组件级评估的优势：**
  * **更高效：** 可以更快地评估单个组件的性能，无需每次都重新运行整个系统。
  * **信号更清晰：** 提供针对特定错误的清晰指标，让开发者确切知道他们是否正在改进目标组件，并避免了整体系统的复杂性带来的噪声。
  * **适用于团队分工：** 如果有多个团队分别负责不同组件，每个团队可以拥有自己的清晰指标，实现更快速、更有针对性的优化。

2. 构建组件级评估的示例（以研究代理的网页搜索为例）

* **问题：** 错误分析表明研究代理遗漏关键点的问题主要出在**网页搜索**组件上。
* **构建评估方法：**
  * **创建基本事实 (Ground Truth)：** 针对少数几个查询，请**人类专家**提供一份**黄金标准网页资源列表 (Gold Standard Web Resources)**，即最权威、最应该找到的网页。
  * **编写评估代码：** 使用信息检索领域的**标准指标**（如 F1 分数），编写代码来衡量**网页搜索的输出列表**与**黄金标准列表**之间的**重叠程度**。
* **用途：**
  * 利用这个指标，开发者可以快速高效地**调整网页搜索组件的参数或超参数**（如更换搜索引擎、更改结果数量、调整日期范围）。
  * **快速实现增量改进**：在调优过程中，可以快速判断网页搜索质量是否提高。

3. 整合组件级评估与端到端评估

* **工作流程：**
  * 通过**错误分析**确定一个问题组件（如网页搜索）。
  * 构建和使用**组件级评估**来**高效地**进行调优和增量改进。
  * 在认为调优完成后，**运行最终的端到端评估**，以**验证**组件的改进确实提升了**整个系统的整体性能**。

4. 下一步：如何改进单个组件
5. ## **【实验】Ungraded Lab: Adding a component-level eval to the research workflow** —— 非评分实验：将组件级评估加入研究流程
6. ## **How to address problems you identify** —— 如何解决已发现的问题

### 改进代理式 AI 工作流程组件的方法与模型选择技巧

这段演讲详细介绍了改进代理式 AI 工作流程中**非 LLM 组件**和 **LLM 组件**的通用方法，并强调了通过经验和观察来培养**模型选择直觉**的重要性。

1. 改进非 LLM 基础组件的方法

非 LLM 组件（如网页搜索、RAG 检索、代码执行、特定 ML 模型等）的改进工具具有多样性。

* **调整参数或超参数：**
  * **网页搜索：** 调整结果数量、日期范围等。
  * **RAG 检索：** 更改相似度阈值、文本块大小（Chunk Size）等。
  * **人物检测：** 调整检测阈值，以权衡误报和漏报。
* **替换组件：** 尝试更换不同的服务提供商（如不同的 RAG 搜索引擎、不同的 Web 搜索 API），以寻找更优的性能。

2. 改进 LLM 基础组件的方法

LLM 组件的改进主要围绕输入、模型本身和工作流程结构展开。

* **改进提示 (Prompting)：**
  * **增加明确指令 (Explicit Instructions)。**
  * **使用少样本提示 (Few-Shot Prompting)：** 添加具体的输入和期望输出示例，帮助 LLM 更好地理解任务。
* **尝试不同的 LLM：** 轻松切换和测试多个 LLM，并使用评估 (Evals) 来选择最适合特定应用的**最佳模型**。
* **任务分解 (Decomposition)：**
  * 如果单个步骤的指令过于复杂，导致 LLM 难以准确执行，考虑将任务分解为**更小的、更易于管理的步骤**（例如：生成步骤 + 反思步骤，或连续多次调用）。
* **微调模型 (Fine-Tuning)：**
  * 这是**最复杂、成本最高**的选项。
  * **时机：** 只有在穷尽所有其他方法后，仍需要**挤出最后几个百分点的性能改进**时才考虑。它适用于更成熟且性能要求极高的应用。

3. 培养模型选择直觉的技巧（关键技能）

拥有对不同 LLM 能力的直觉，能使开发者更高效地选择模型和编写提示。

* **观察模型特点：****较大的前沿模型**通常**更擅长遵循复杂的指令**，而较小的模型可能只擅长回答简单的事实性问题。
  * *案例：* 在**编辑 PII**（遵循多步骤复杂指令）的任务中，智能程度更高的模型能够完美地列出并编辑所有敏感信息，而较小的模型容易出错或遗漏信息。
* **培养直觉的方法：**
  * **频繁试玩不同模型：** 经常测试新的闭源和开源模型，观察它们在不同查询上的表现。
  * **建立个人评估集：** 使用一套固定的评估任务来校准不同模型的能力。
  * **阅读他人的提示 (Prompts)：** 大量阅读同行、专家或开源包中的提示，以了解提示的最佳实践，从而提高自己编写提示的能力。
  * **在工作流程中尝试：** 实际在代理式工作流程中尝试不同的模型，通过**查看追踪 (Traces)** 和**组件/端到端评估**，观察它们在特定任务中的性能、价格和速度的权衡。

4. 下一步优化方向

在系统输出质量达到要求后，下一个优化重点是：**优化工作流程的延迟 (Latency) 和成本 (Cost)**。

7. ## **Latency, cost optimization** —— 延迟与成本优化

### 代理式 AI 工作流程的成本与延迟优化

这段演讲讨论了在代理式 AI 工作流程中，何时以及如何系统性地优化**延迟 (Latency)** 和**成本 (Cost)**。

1. 优化时机的建议

* **优先顺序：** 演讲者建议团队首先应**专注于获得高质量的输出 (Output Quality)**。
* **原因：** 提升输出质量通常是开发中最困难的部分。只有当系统**运行良好并被大量使用**时，才应该将精力转向成本和延迟优化。
* **“好的问题”：** 只有当系统被大量用户使用，以至于成本成为问题时，才是开始集中优化成本的理想时机。

2. 优化延迟（速度）的方法

优化延迟的关键在于进行**计时基准测试 (Benchmarking)**，找出工作流程中的瓶颈。

* **计时分析：** 详细记录工作流程中**每个步骤所花费的时间**（例如：LLM 1 耗时 7 秒，LLM 2 耗时 18 秒）。
* **定位瓶颈：** 通过时间线分析，确定**耗时最长**的组件，从而确定最大的提速空间。
* **优化手段：**
  * **并行化 (Parallelism)：** 考虑将一些可以独立进行的步骤（如网页抓取）**并行执行**。
  * **更换 LLM：** 尝试使用**更小、更快**（尽管可能稍不智能）的模型，或者测试**不同的 LLM 提供商**，以找到**返回 token 最快**的服务。

3. 优化成本的方法

优化成本的关键在于进行**成本基准测试 (Cost Benchmarking)**，找出最昂贵的步骤。

* **成本计算：** 计算工作流程中**每个步骤的平均成本**：
  * **LLM：** 按**输入和输出的 Token 长度**收费。
  * **API：** 按**调用次数**收费。
  * **计算/服务：** 根据服务器容量、服务费等计算。
* **定位瓶颈：** 确定**成本贡献最大**的组件。
* **优化手段：** 寻找**更便宜的组件或 LLM** 来替代高成本的组件，以最大化成本优化机会。

4. 总结与决策基础

* **核心作用：** 无论是延迟还是成本，通过**测量每一步的指标**（计时和成本），可以清晰地确定**哪些组件是主要贡献者**，从而提供一个**决定优化工作重点**的系统性基础。
* **效率：** 这种基准测试练习可以迅速识别那些对成本或延迟**贡献不大的组件**，避免不必要的优化工作。

8. ## **Development process summary** —— 开发流程总结

### 详细总结：代理式 AI 系统开发流程与迭代循环

这段演讲总结了构建代理式 AI 系统的**非线性迭代过程**，强调了**分析 (Analysis)** 在指导开发工作中的核心地位。

1. 开发工作中的两大核心活动

开发者在构建代理式工作流程时，主要精力集中在以下两项活动中，并不断**来回切换**：

1. **构建 (Building)：** 编写软件和代码来改进系统。
2. **分析 (Analyzing)：** 决定下一步将构建精力集中在哪里的过程，其重要性与构建相当。
3. 系统开发和分析的成熟度阶段（迭代循环）

系统从初始原型到成熟，分析工作的严谨性也随之提高，通常经历以下迭代阶段：

| 阶段        | 描述                                                | 主要活动                                                                                         |
| ----------- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| 1. 快速原型 | 快速构建一个端到端系统（“Quick and Dirty”实现）。 | 分析： 手动检查最终输出，通读追踪 (Traces)，凭直觉找出性能不佳的组件。                           |
| 2. 初步评估 | 系统开始成熟，超越纯手动观察。                      | 分析： 构建初步的 端到端评估 (Evals)，使用小型数据集（如 10-20 例）计算整体性能指标。            |
| 3. 严谨分析 | 系统需要更精确的改进方向。                          | 分析： 进行错误分析 (Error Analysis)，统计和量化各个组件导致次优输出的频率，以做出更集中的决策。 |
| 4. 高效调优 | 系统进一步成熟，需要在组件级别进行高效改进。        | 分析： 构建组件级评估 (Component-Level Evals)，以便更高效地对单个组件进行调优。                  |

**核心观点：** 开发是一个**非线性**的过程，需要在调整系统、进行错误分析、改进组件和调整评估之间**来回跳跃 (bouncing back and forth)**。

2. 经验总结与工具使用

* **经验不足团队的问题：** 往往花**太多时间在构建上**，而**太少时间在分析上**，这导致工作重点不集中，效率低下。
* **工具的使用：** 市面上有许多工具可以帮助监控追踪、记录运行时、计算成本等。
* **定制化需求：** 尽管可以使用通用工具，但由于大多数代理式工作流程是高度定制化的，开发者最终仍需要**自行构建许多定制化的评估 (Custom Evals)**，以准确捕获系统特有的问题。

3. 结语

掌握本模块介绍的系统化流程（如错误分析、不同级别的评估）后，开发者在实施代理式工作流程的复杂程度上将**领先于绝大多数开发者**。在第五节，课程将介绍更高级的设计模式。


